---
title: "03 Automated Machine Learning with H20"
date: "2021-06-02"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

# Business case study
```{r}
# Libraries 
library(tidyverse)
library(readxl)
library(skimr)
library(GGally)
library(rsample)

# Load Data data definitions
employee_attrition_tbl <- read_csv("./01_ml_fund_source/Business\ Decisions\ with\ Machine\ Learning/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv")


path_data_definitions <- "./01_ml_fund_source/Business\ Decisions\ with\ Machine\ Learning/data_definitions.xlsx"
definitions_raw_tbl   <- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)

employee_attrition_tbl
```
```{r}
# Business & Data Understanding: Department and Job Role

# Data subset
dept_job_role_tbl <- employee_attrition_tbl %>%
  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)

dept_job_role_tbl %>%

  group_by(Attrition) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(pct = n / sum(n))

# Attrition by department
dept_job_role_tbl %>%

  # Block 1
  group_by(Department, Attrition) %>%
  summarize(n = n()) %>%
  ungroup() %>%

  # Block 2: Caution: It's easy to inadvertently miss grouping when creating counts & percents within groups
  group_by(Department) %>%
  mutate(pct = n / sum(n))

# Attrition by job role
dept_job_role_tbl %>%

  # Block 1
  group_by(Department, JobRole, Attrition) %>%
  summarize(n = n()) %>%
  ungroup() %>%

  # Block 2
  group_by(Department, JobRole) %>%
  mutate(pct = n / sum(n)) %>%
  ungroup() %>%

  # Block 3
  filter(Attrition %in% "Yes")
```
```{r}
# Develop KPI

dept_job_role_tbl %>%

  # Block 1
  group_by(Department, JobRole, Attrition) %>%
  summarize(n = n()) %>%
  ungroup() %>%

  # Block 2
  group_by(Department, JobRole) %>%
  mutate(pct = n / sum(n)) %>%
  ungroup() %>%

  # Block 3
  filter(Attrition %in% "Yes") %>%
  arrange(desc(pct)) %>%
  mutate(
    above_industry_avg = case_when(
      pct > 0.088 ~ "Yes",
      TRUE ~ "No"
    )
  )
```
```{r}
# Function to calculate attrition cost
calculate_attrition_cost <- function(

  # Employee
  n                    = 1,
  salary               = 80000,

  # Direct Costs
  separation_cost      = 500,
  vacancy_cost         = 10000,
  acquisition_cost     = 4900,
  placement_cost       = 3500,

  # Productivity Costs
  net_revenue_per_employee = 250000,
  workdays_per_year        = 240,
  workdays_position_open   = 40,
  workdays_onboarding      = 60,
  onboarding_efficiency    = 0.50

) {

  # Direct Costs
  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)

  # Lost Productivity Costs
  productivity_cost <- net_revenue_per_employee / workdays_per_year *
    (workdays_position_open + workdays_onboarding * onboarding_efficiency)

  # Savings of Salary & Benefits (Cost Reduction)
  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open

  # Estimated Turnover Per Employee
  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction

  # Total Cost of Employee Turnover
  total_cost <- n * cost_per_employee

  return(total_cost)

}

calculate_attrition_cost()
calculate_attrition_cost(200)
```

```{r}
# Use this
# Function to convert counts to percentages. 
count_to_pct <- function(data, ..., col = n) {

  # capture the dots
  grouping_vars_expr <- quos(...)
  col_expr <- enquo(col)

  ret <- data %>%
    group_by(!!! grouping_vars_expr) %>%
    mutate(pct = (!! col_expr) / sum(!! col_expr)) %>%
    ungroup()

  return(ret)

}

# This is way shorter and more flexibel
dept_job_role_tbl %>%
  count(JobRole, Attrition) %>%
  count_to_pct(JobRole)
  
dept_job_role_tbl %>%
  count(Department, JobRole, Attrition) %>%
  count_to_pct(Department, JobRole)  

```
```{r}
assess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {

  attrition_col_expr <- enquo(attrition_col)

  data %>%
  
    # Use parenthesis () to give tidy eval evaluation priority
    filter((!! attrition_col_expr) %in% attrition_value) %>%
    arrange(desc(pct)) %>%
    mutate(
      # Function inputs in numeric format (e.g. baseline_pct = 0.088 don't require tidy eval)
      above_industry_avg = case_when(
        pct > baseline_pct ~ "Yes",
        TRUE ~ "No"
      )
    )

}

dept_job_role_tbl %>%

  count(Department, JobRole, Attrition) %>%
  count_to_pct(Department, JobRole) %>%
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>%
  mutate(
    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)
  )
```
```{r}
dept_job_role_tbl %>%

  group_by(Department, JobRole, Attrition) %>%
  summarize(n = n()) %>%
  ungroup() %>%

  group_by(Department, JobRole) %>%
  mutate(pct = n / sum(n)) %>%
  ungroup() %>%

  filter(Attrition %in% "Yes") %>%
  arrange(desc(pct)) %>%
  mutate(
    above_industry_avg = case_when(
      pct > 0.088 ~ "Yes",
      TRUE ~ "No"
    )
  ) %>%

  mutate(
    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)
  )
```
```{r}
dept_job_role_tbl %>%

  count(Department, JobRole, Attrition) %>%
  count_to_pct(Department, JobRole) %>%
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>%
  mutate(
    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)
  ) %>%

  # Data Manipulation
  mutate(name = str_c(Department, JobRole, sep = ": ") %>% as_factor()) %>%

  # Check levels
  # pull(name) %>%
  # levels()

  mutate(name      = fct_reorder(name, cost_of_attrition)) %>%
  mutate(cost_text = str_c("$", format(cost_of_attrition / 1e6, digits = 2),
                           "M", sep = "")) %>%

  #Plotting
  ggplot(aes(cost_of_attrition, y = name)) +
  geom_segment(aes(xend = 0, yend = name),    color = "#2dc6d6") +
  geom_point(  aes(size = cost_of_attrition), color = "#2dc6d6") +
  scale_x_continuous(labels = scales::dollar) +
  geom_label(aes(label = cost_text, size = cost_of_attrition),
             hjust = "inward", color = "#2dc6d6") +
  scale_size(range = c(3, 5)) +
  labs(title = "Estimated cost of Attrition: By Dept and Job Role",
       y = "",
       x = "Cost of attrition") +
  theme(legend.position = "none")
```


```{r}
# Function to plot attrition
plot_attrition <- function(data, 
                           ..., 
                           .value,
                           fct_reorder = TRUE,
                           fct_rev     = FALSE,
                           include_lbl = TRUE,
                           color       = "#2dc6d6",
                           units       = c("0", "K", "M")) {

  ### Inputs
  group_vars_expr   <- quos(...)
  
  # If the user does not supply anything, 
  # this takes the first column of the supplied data
  if (length(group_vars_expr) == 0) {
    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))
    }

  value_expr <- enquo(.value)

  units_val  <- switch(units[[1]],
                       "M" = 1e6,
                       "K" = 1e3,
                       "0" = 1)
  if (units[[1]] == "0") units <- ""

  # Data Manipulation
  # This is a so called Function Factory (a function that produces a function)
  usd <- scales::dollar_format(prefix = "$", largest_with_cents = 1e3)

  # Create the axis labels and values for the plot
  data_manipulated <- data %>%
    mutate(name = str_c(!!! group_vars_expr, sep = ": ") %>% as_factor()) %>%
    mutate(value_text = str_c(usd(!! value_expr / units_val),
                              units[[1]], sep = ""))

  
  # Order the labels on the y-axis according to the input
  if (fct_reorder) {
    data_manipulated <- data_manipulated %>%
      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>%
      arrange(name)
  }

  if (fct_rev) {
    data_manipulated <- data_manipulated %>%
      mutate(name = forcats::fct_rev(name)) %>%
      arrange(name)
  }

  # Visualization
  g <- data_manipulated %>%

        # "name" is a column name generated by our function internally as part of the data manipulation task
        ggplot(aes(x = (!! value_expr), y = name)) +
        geom_segment(aes(xend = 0, yend = name), color = color) +
        geom_point(aes(size = !! value_expr), color = color) +
        scale_x_continuous(labels = scales::dollar) +
        scale_size(range = c(3, 5)) +
        theme(legend.position = "none")

  # Plot labels if TRUE
  if (include_lbl) {
    g <- g +
      geom_label(aes(label = value_text, size = !! value_expr),
                 hjust = "inward", color = color)
  }

  return(g)

}


dept_job_role_tbl %>%

  # Select columnns
  count(Department, JobRole, Attrition) %>%
  count_to_pct(Department, JobRole) %>%
  
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>%
  mutate(
    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)
  ) %>%

  # Select columnns
  plot_attrition(Department, JobRole, .value = cost_of_attrition,
                 units = "M") +
  labs(
    title = "Estimated Cost of Attrition by Job Role",
    x = "Cost of Attrition",
    subtitle = "Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost"
  )
```

```{r}
# Step 1: Data Summarization -----

skim(employee_attrition_tbl)

# Character Data Type
employee_attrition_tbl %>%
    select_if(is.character) %>%
    glimpse()

# Get "levels"
employee_attrition_tbl %>%
    select_if(is.character) %>%
    map(unique)

# Proportions    
employee_attrition_tbl %>%
    select_if(is.character) %>%
    map(~ table(.) %>% prop.table())

# Numeric Data
employee_attrition_tbl %>%
    select_if(is.numeric) %>%
    map(~ unique(.) %>% length())

employee_attrition_tbl %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>%
    # Select all columns
    pivot_longer(everything()) %>%
    arrange(value) %>%
    filter(value <= 10)
```

```{r}
# Step 2: Data Visualization ----

employee_attrition_tbl %>%
    select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%
    ggpairs() 
```
```{r}
employee_attrition_tbl %>%
    select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%
    ggpairs(aes(color = Attrition), lower = "blank", legend = 1,
            diag  = list(continuous = wrap("densityDiag", alpha = 0.5))) +
    theme(legend.position = "bottom")
```

```{r}
plot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {
    
    color_expr <- enquo(color)
    
    if (rlang::quo_is_null(color_expr)) {
        
        g <- data %>%
            ggpairs(lower = "blank") 
        
    } else {
        
        color_name <- quo_name(color_expr)
        
        g <- data %>%
            ggpairs(mapping = aes_string(color = color_name), 
                    lower = "blank", legend = 1,
                    diag = list(continuous = wrap("densityDiag", 
                                                  alpha = density_alpha))) +
            theme(legend.position = "bottom",
                  text = element_text(size=8),
                  axis.text = element_text(size = 10),
                  axis.title = element_text(size = 10))
    }
    
    return(g)
    
}

```


```{r}
employee_attrition_tbl %>%
    select(Attrition,  contains("Training")) %>%
    plot_ggpairs(Attrition)
```

# Challenge 1

Use your learning from descriptive features and plot_ggpairs() to further investigate the features. Run the functions above according to the features needed. Answer the following questions. Most of the time, you will only need the images from diagonal.

***

1. What can you deduce about the interaction between Monthly Income and Attrition?  <font color=red size=5>c</font>

a. Those that are leaving the company have a higher Monthly Income
b. That those are staying have a lower Monthly Income
c. Those that are leaving have a lower Monthly Income
d. It's difficult to deduce anything based on the visualization

***

2. What can you deduce about the interaction between Percent Salary Hike and Attrition? <font color=red size=5>d</font>

a. Those that are leaving the company have a higher Percent Salary Hike
b. Those that are staying have a lower Percent Salary Hike
c. Those that are leaving have lower Percent Salary Hike
d. It's difficult to deduce anything based on the visualization

***

3. What can you deduce about the interaction between Stock Option Level and Attrition? <font color=red size=5>b</font>

a. Those that are leaving the company have a higher stock option level
b. Those that are staying have a higher stock option level
c. It's difficult to deduce anything based on the visualization

***

4. What can you deduce about the interaction between Environment Satisfaction and Attrition? <font color=red size=5>a</font>

a. A higher proportion of those leaving have a low environment satisfaction level
b. A higher proportion of those leaving have a high environment satisfaction level
c. It's difficult to deduce anything based on the visualization

***

5. What can you deduce about the interaction between Work Life Balance and Attrition <font color=red size=5>b</font>

a. Those that are leaving have higher density of 2's and 3's
b. Those that are staying have a higher density of 2's and 3's
c. Those that are staying have a lower density of 2's and 3's
d. It's difficult to deduce anything based on the visualization

***

6. What Can you deduce about the interaction between Job Involvement and Attrition? <font color=red size=5>a</font>

a. Those that are leaving have a lower density of 3's and 4's
b. Those that are leaving have a lower density of 1's and 2's
c. Those that are staying have a lower density of 2's and 3's
d. It's difficult to deduce anything based on the visualization

***

7. What can you deduce about the interaction between Over Time and Attrition? <font color=red size=5>a</font>

a. The proportion of those leaving that are working Over Time are high compared to those that are not leaving
b. The proportion of those staying that are working Over Time are high compared to those that are not staying

***

8. What can you deduce about the interaction between Training Times Last Year and Attrition <font color=red size=5>b</font>

a. People that leave tend to have more annual trainings
b. People that leave tend to have less annual trainings
c. It's difficult to deduce anything based on the visualization

***

9. What can you deduce about the interaction between Years At Company and Attrition <font color=red size=5>b</font>
 
a. People that leave tend to have more working years at the company
b. People that leave tend to have less working years at the company
c. It's difficult to deduce anything based on the visualization

***

10. What can you deduce about the interaction between Years Since Last Promotion and Attrition? <font color=red size=5>c</font>

a. Those that are leaving have more years since last promotion than those that are staying
b. Those that are leaving have fewer years since last promotion than those that are staying
c. It's difficult to deduce anything based on the visualization


# Challenge 2
## Load the training & test dataset
```{r}
library(tidyverse)
# Modeling
library(parsnip)
# Pre-processing & Sampling
library(recipes)
library(rsample)
# Modeling Error Metrics
library(yardstick)
library(workflows)
library(tune)

product_data <- read_csv("./01_ml_fund_source/Business\ Decisions\ with\ Machine\ Learning/product_backorders.csv")
product_data2 <- product_data %>% 
  mutate(
      product_backorder = went_on_backorder %>% str_to_lower() %>% str_detect("yes") %>% as.numeric()
  ) %>% 
  select(-c(went_on_backorder))
glimpse(product_data)

split_obj<- initial_split(product_data2, prop = 0.75)
train_tbl<- training(split_obj)
test_tbl<- testing(split_obj)
```
## Specifiy the response and predictor variables
```{r}
recipe_obj <- recipe(product_backorder ~., data = train_tbl) %>% 
    step_zv(all_predictors()) %>% 
    step_dummy(all_nominal(),-all_outcomes()) %>%
    prep()
```

```{r}
summary(recipe_obj)
```
```{r}
glimpse(bake(recipe_obj,new_data = NULL))

```
## run AutoML specifying the stopping criterion
```{r}
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home")
Sys.setenv(https_proxy="") 
Sys.setenv(http_proxy="") 
Sys.setenv(http_proxy_user="") 
Sys.setenv(https_proxy_user="")

library(h2o)
h2o.init()
```

```{r}
# Split data into a training and a validation data frame
# Setting the seed is just for reproducability
split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.75), seed = 42)
train_h2o <- split_h2o[[1]]
valid_h2o <- split_h2o[[2]]
test_h2o  <- as.h2o(test_tbl)

# Set the target and predictors
y <- "product_backorder"
x <- setdiff(names(train_h2o), y)
```

```{r}
automl_models_h2o <- h2o.automl(
  x = x,
  y = y,
  training_frame    = train_h2o,
  validation_frame  = valid_h2o,
  leaderboard_frame = test_h2o,
  max_runtime_secs  = 120,
  nfolds            = 5,
  stopping_metric = "mae", stopping_rounds = 3,
                        stopping_tolerance = 1e-2
)

```
## View the leaderboard
```{r}
#slotNames(automl_models_h2o)
automl_models_h2o@leaderboard 


extract_h2o_model_name_by_position <- function(h2o_leaderboard, n = 1, verbose = T) {
    
    model_name <- h2o_leaderboard %>%
        as.tibble() %>%
        slice_(n) %>%
        pull(model_id)
    
    if (verbose) message(model_name)
    
    return(model_name)
    
}

```



## Predicting using Leader Model
```{r}
best_model <- automl_models_h2o@leaderboard %>% 
  extract_h2o_model_name_by_position(1) %>% 
  h2o.getModel()

predictions <- h2o.predict(best_model, newdata = as.h2o(test_tbl))

typeof(predictions)

predictions_tbl <- predictions %>% as_tibble()

glimpse(predictions_tbl)

```

## Save the leader model
```{r}
best_model %>% h2o.saveModel(path = "StackedEnsemble_AllModels_AutoML_20210603_202714")
```
